---
title: "BIS 537 Final Project Report"
author: "Can Meng and Waveley Qiu"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
geometry: margin=2cm
output: 
  bookdown::pdf_document2:
    toc: false
    number_sections: false
fontsize: 11pt
header-includes:
  - \usepackage{setspace}\doublespacing
  - \usepackage{mathrsfs}
  - \usepackage{amsfonts}  
  - \usepackage{amsmath}
  - \usepackage{amsthm}
  - \usepackage{amssymb,,bbm,graphics,mathrsfs}
  - \usepackage{geometry}
  - \usepackage{tikz}
  - \usetikzlibrary{shapes,arrows,decorations}
---

```{r setup, include=FALSE}
set.seed(20220417)
library(tidyverse)
library(bookdown)
library(latex2exp)
library(progress)
library(patchwork)
library(kableExtra)

# set knitr defaults
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.align = "center",
  fig.width = 6,
  fig.asp   = .5,
  out.width = "90%",
  cache = FALSE
)

# set theme defaults
theme_set(
  theme_bw() +
    theme(
      legend.position = "bottom",
      plot.title    = element_text(hjust = 0.5),
      plot.subtitle = element_text(hjust = 0.5),
      plot.caption  = element_text(hjust = 0.0)
    )
)

# set color scale defaults
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill   = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete   = scale_fill_viridis_d
```

# 1. Introduction

While the ideal setting to conduct causal inference on survival outcomes are randomized experiments, as is true for most areas of research, it is often the case that such an experimental setting is difficult or impossible to achieve. As such, when treatment and control groups are not exchangeable, it may be necessary to implement causal inference methods to address the absence of direct counterfactual proxies.

Time-to-event outcomes are of great interest to investigators across a variety of clinical settings, however present analytical difficulties even within the constrains of a randomized controlled trial. In particular, the non-observation of an event yet loss of a subject's data, or censoring, presents another missing data issue that the investigator has need to consider. This issue is compounded in the setting of a non-randomized or observational study, where exchangeability between treatment groups cannot be assumed of the collected data.

Several procedures addressing this dual-level missing data issue in the estimation of time-to-event causal effects have been explored previously. Some of these procedures extend the idea of inverse probability weighting to the setting of survival analysis in constructing a combined treatment and censoring weights (Hernan 2000, Cheng 2022) to approximate exchangeability and estimating the causal estimand through a weighted estimator. Other methods approach the problem by obtaining survival outcomes for each subject through a jackknife procedure (Andersen 2010) and estimate the causal estimand based on those pseudo-observed endpoints. The pseudo-observaions approach has been further improved upon with the employment of propensity score weighting to close the distance between the collected sample and the target population (Li 2021) as well as with the recent proposal of a doubly-robust estimation technique (Wang 2023) to allow for some flexibility with regards to model misspecification. 

In this study, we are interested in revisiting the weighting methods and are proposing a new procedure in which model misspecification may be implicitly adjusted for. In doing so, we will first employ the two-arm weighting method established by Cheng et.al (2022), in which weights based on the probability of treatment and the probability of censoring are calculated and used together in estimating the causal estimand of interest. We will then compare its performance to the results obtained from a single-arm sequential weighting method that we have developed, where weights are constructed iteratively, and the final compositely-weighted dataset will then be used to estimate the causal estimand.

The remainder of this paper is organized as follows: in section 2, we introduce the methods developed and utilized in this study; in section 3, we discuss our data generation procedure and simulation design; in section 4, we illustrate our results from the conducted simulation study; and section 5 concludes with a discussion.

# 2. Methods

## 2.1 Causal Survival Estimand

While there are several causal estimands that could be of interest, we will be focusing our study on the restricted average causal effect (RACE) for ease of interpretation. In Mao et al. (2018), this was defined as follows:
$$
\begin{aligned}
  \Delta_{RACE} &= \frac{E[\omega(e_i)\min(T_{1i}, t^*)]}{E[\omega(e_i)]} - \frac{E[\omega(e_i)\min(T_{0i}, t^*)]}{E[\omega(e_i)]}
  \\
  &= \int_0^{t^*} S_1(t)dt - \int_0^{t^*} S_0(t)dt
\end{aligned}
$$

This estimand is interpreted as the average difference in survival time between the treatment and control groups, if both potential outcomes are observed, under the upper bound time restriction of $t^*$.

In this study, we are interested in the upper bound time restriction of 5 -- thus, our estimand will take the following form:
$$
\begin{aligned}
  \Delta_{RACE} &= \frac{E[\omega(e_i)\min(T_{1i}, 5)]}{E[\omega(e_i)]} - \frac{E[\omega(e_i)\min(T_{0i}, 5)]}{E[\omega(e_i)]}
  \\
  &= \int_0^{5} S_1(t)dt - \int_0^{5} S_0(t)dt
\end{aligned}
$$

## 2.2 Causal Estimators

In the separate weighting procedure, to incorporate both the inverse probability treatment weights (IPTW) and the inverse probability of censoring weights (IPCW), we employed the usage of the non-parametric weighted Kaplan-Meier estimator defined by Cheng et al. (2022). This estimator is defined as follows:
$$
\begin{aligned}
  \hat{\Delta}(t)&=\hat{S}(1)-\hat{S}(0)
  \\
  &=\Bigg(1-\frac{\sum_{i=1}^{n}\omega_{IPTW_i}Z_i\delta_iI(U_i\leq t)\omega_{IPCW_i}}{\sum_{i=1}^{n}\omega_{IPTW_i}Z_i}\Bigg)-\Bigg(1-\frac{\sum_{i=1}^{n}\omega_{IPTW_i}(1-Z_i)\delta_iI(U_i\leq t)\omega_{IPCW_i}}{\sum_{i=1}^{n}\omega_{IPTW_i}(1-Z_i)}\Bigg)
  \end{aligned}
$$

where $T_i^{S}$ is the event time of interest, $T_i^{C}$ is the censoring time, $U_i=min(T_i^{S}, T_i^{C})$ is the observed time in study, $\delta_i$ is the censoring indicator, $Z_i$ is the treatment indicator, $I(U_i\leq t)$ is the indicator for the observed time being less than the upper limit of time $t$, and $\omega_{IPTW_i}$ and $\omega_{IPCW_i}$ are IPTW and IPCW respectively. Then, our estimator would be defined as follows, at $t = 5$: 
$$
\begin{aligned}
  \hat{\Delta}(5)&= \Bigg(1-\frac{\sum_{i=1}^{n}\omega_{IPTW_i}Z_i\delta_iI(U_i\leq 5)\omega_{IPCW_i}}{\sum_{i=1}^{n}\omega_{IPTW_i}Z_i}\Bigg)
  -\Bigg(1-\frac{\sum_{i=1}^{n}\omega_{IPTW_i}(1-Z_i)\delta_iI(U_i\leq 5)\omega_{IPCW_i}}{\sum_{i=1}^{n}\omega_{IPTW_i}(1-Z_i)}\Bigg)
\end{aligned}
$$ 

The estimators used in the sequentially weighted procedure will similarly be weighted Kaplan-Meier estimators -- however, these will be distinct from the previously defined estimator in that there will be the need to adjust for one set of weights only. The estimator will thus have the following form, both generally and at $t = 5$:
$$
\begin{aligned}
  \hat{\Delta}(t)&=\hat{S}(1)-\hat{S}(0)
  \\
  &=\Bigg(1-\frac{\sum_{i=1}^{n}\omega_{IPTW\circ IPCW_i}Z_i\delta_iI(U_i\leq t)}{\sum_{i=1}^{n}\omega_{IPTW\circ IPCW_i}Z_i}\Bigg)-\Bigg(1-\frac{\sum_{i=1}^{n}\omega_{IPTW\circ IPCW_i}(1-Z_i)\delta_iI(U_i\leq t)}{\sum_{i=1}^{n}\omega_{IPTW\circ IPCW_i}(1-Z_i)}\Bigg)
  \\
  \hat{\Delta}(5)&=\hat{S}(1)-\hat{S}(0)
  \\
  &=\Bigg(1-\frac{\sum_{i=1}^{n}\omega_{IPTW\circ IPCW_i}Z_i\delta_iI(U_i\leq 5)}{\sum_{i=1}^{n}\omega_{IPTW\circ IPCW_i}Z_i}\Bigg)-\Bigg(1-\frac{\sum_{i=1}^{n}\omega_{IPTW\circ IPCW_i}(1-Z_i)\delta_iI(U_i\leq 5)}{\sum_{i=1}^{n}\omega_{IPTW\circ IPCW_i}(1-Z_i)}\Bigg)
\end{aligned}
$$

where $\omega_{IPTW\circ IPCW_i}$ is the composite treatment and censoring weight and all other notation is the same as defined previously.

## 2.3 Weighting Procedures

Critical to the construction of the causal estimators are the weighting procedures underlying the construction of the inverse probability weights of both treatment assignment and censoring indication. The goal for both of the procedures that are the subject of investigation in this study is covariate balance between treatment and control groups, such that the study sample would mimic the distribution of the theoretical larger population that it is drawn from. 

All weights constructed in this study were inverse probability weights -- in particular, for the construction of a weight on indicator variable $R$, the weights $w_{IPRW_i}$ for a given subject $i$ were constructed as follows:

$$
\begin{aligned}
\mathbb{E}[R|\mathbf{X}] &= e_{R}(\mathbf{X}), \quad\text{ where }\mathbf{X} \text{ is the vector of covariate values}
\\
w_{IPRW_i} &= \begin{cases}
\frac{1}{e_R(\mathbf{X})}, \quad\text{ if R = 1}
\\
\frac{1}{1 - e_R(\mathbf{X})}, \quad\text{ if R = 0}
\end{cases}
\end{aligned}
$$

In the estimated propensity score models, all available baseline covariates (which does exclude treatment assignment and censoring indication) were included as model predictors, irrespective of how the true propensity score model was generated. Though this by default produces a misspecified model, our intent was to replicate and examine situations that may occur in practice, where the data generation procedure is unknown and it may seem reasonable to the investigator to include all collected baseline characteristics. We will explore the impact of this decision in the Discussion section of this paper.  

### 2.3.1 Separate Weighting

The first weighting procedure is to estimate IPTW and IPCW separately. Then the two weights are used together in the estimator described in the previous section to estimate our target estimand (see the diagram below).

```{tikz, echo = FALSE}
\tikzstyle{block} = [rectangle, draw, text width=6em, text centered, rounded corners, minimum height=4em]
\tikzstyle{calculation} = [circle, draw, text width=4em, text centered, minimum height=4em]
\tikzstyle{result} = [rectangle, draw, text width=7em, text centered, minimum height=4em]

\tikzstyle{line} = [draw, -latex]
\tikzstyle{doublearr}=[latex-latex, black, line width=0.5pt]
  \begin{tikzpicture}[auto]
      % Place nodes
      \node [block] (D) {Observed Data};
      \node [calculation, right of = D, xshift = 2cm, yshift = 1cm] (IPTW) {IPTW};
      \node [calculation, right of = D, xshift = 2cm, yshift = -1cm] (IPCW) {IPCW};
      \node [block, right of = D, xshift = 5cm] (WD) {IPTW $\times$ IPCW*};
      \node [block, text width = 3cm, right of = WD, xshift = 3cm] (EY) {$\hat{E}[S_1(5) - S_0(5)]$};
      % Draw edges
      \path [line] (D) -- (IPTW);
      \path [line] (D) -- (IPCW);
      \path [line] (IPTW) -- (WD);
      \path [line] (IPCW) -- (WD);
      \path [line] (WD) -- (EY);
  \end{tikzpicture}
```

In the separate weighting procedure, IPTW was estimated solely using logistic regression, and IPCW was estimated using both logistic regression and Cox-Exponential regression. The logistic regressions were fitted using `glm` function and the Cox-Exponential models were fit using `ipcw` from the `riskRegression` R package.

### 2.3.2 Sequential Weighting

In this second weighting procedure, weights are first constructed based on either treatment assignment or censoring indication propensity scores. After this, the weighted dataset would be used to generate weights based on the indication that was not used in the first step of weighting. The causal estimand would be estimated using the second set of weights applied to to the weighted dataset from the first step of weighting.

The weighting processes were conducted as depicted in the following diagram, where circles indicate where weighting occurred and boxes marked with asterisks indicate pseudo-populations:

```{tikz, echo = FALSE}
\tikzstyle{block} = [rectangle, draw, text width=6em, text centered, rounded corners, minimum height=4em]
\tikzstyle{calculation} = [circle, draw, text width=4em, text centered, minimum height=4em]
\tikzstyle{result} = [rectangle, draw, text width=7em, text centered, minimum height=4em]

\tikzstyle{line} = [draw, -latex]
\tikzstyle{doublearr}=[latex-latex, black, line width=0.5pt]
  \begin{tikzpicture}[auto]
      % Place nodes
      \node [block] (D) {Observed Data};
      \node [calculation, right of = D, xshift = 1.5cm] (IPTW) {IPTW};
      \node [block, right of = IPTW, xshift = 1.5cm] (WD1) {IPTW*};
      \node [calculation, right of = WD1, xshift = 1.5cm] (IPCW) {IPCW};
      \node [block, right of = IPCW, xshift = 1.5cm] (WD2) {IPCW $\circ$ IPTW*};
      \node [block, text width = 3cm, right of = WD2, xshift = 2.5cm] (EY) {$\hat{E}[S_1(5) - S_0(5)]$};

      % second set of nodes

      \node [block, below of = D, yshift = 4cm] (DR) {Observed Data};
      \node [calculation, right of = DR, xshift = 1.5cm] (IPCWR) {IPCW};
      \node [block, right of = IPCWR, xshift = 1.5cm] (WD1R) {IPCW*};
      \node [calculation, right of = WD1R, xshift = 1.5cm] (IPTWR) {IPTW};
      \node [block, right of = IPTWR, xshift = 1.5cm] (WD2R) {IPTW $\circ$ IPCW*};
      \node [block, text width = 3cm, right of = WD2R, xshift = 2.5cm] (EYR) {$\hat{E}[S_1(5) - S_0(5)]$};

  % Draw edges
      \path [line] (D) -- (IPTW);
      \path [line] (IPTW) -- (WD1);
      \path [line] (WD1) -- (IPCW);
      \path [line] (IPCW) -- (WD2);
      \path [line] (WD2) -- (EY);

      \path [line] (DR) -- (IPCWR);
      \path [line] (IPCWR) -- (WD1R);
      \path [line] (WD1R) -- (IPTWR);
      \path [line] (IPTWR) -- (WD2R);
      \path [line] (WD2R) -- (EYR);
  \end{tikzpicture}
```

We have considered the utility of both indicator variables (treatment assignment and censoring indication) as weighting starting points because the relationship between censoring indication and treatment assignment causes it to be difficult to favor one sequential weighting pathway over the other. Some may prefer the sequential weighting procedure that calculates treatment weights first, on the basis of observational temporality -- however, while it is true that treatment assignment is observed by the researcher prior to censoring time, if the subject indeed is censored, it seems that the timeline of the conduct of the actual study may not necessarily be the best guideline in determining the order of how weights are calculated, especially if it is assumed that censoring time and treatment assignment are independent of each other, as is commonly assumed and has been maintained in this study. In such a case, when censoring time is a quantity that is generated without respect to treatment assignment, the censoring indicator variable would be related to treatment assignment indirectly, through the effect that the assigned treatment would have on survival time; this is a rather complex relationship to evaluate 

Thus, in this study, we have considered and presented results for both options of sequential weighting, allowing room for investigators interested in this method of inverse probability weighting adjustment to select the method most suitable for the parameters of their studies. 

In the implementation of both procedures, the treatment weights were obtained through a logistic regression in `R`, using the `glm` function in base `R` with a logit link function, and the censoring weights were obtained through two Cox-Exponential regressions, which were separated by treatment group, through the `ipcw` function in the `riskRegression` package. In the establishment of the IPTW $\circ$ IPCW procedure, the censoring weights were directly identified in the `glm` function call. In the establishment of the IPCW $\circ$ IPTW procedure, as the `ipcw` function does not take in weights, the treatment-weighted dataset was expanded, using the `expandRows` function from the `splitstackshape` package, according to the calculated treatment weights in order to obtain the second set of weights for censoring indication. 

# 3. Data Generation

We will generate data in accordance with the following relationships, and the causal effect of interest is highlighted in red:

```{tikz, echo = FALSE, fig.height = 4, fig.asp = 0.2}
\tikzstyle{block} = [rectangle, draw, text width=6em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex]
\tikzstyle{doublearr}=[latex-latex, black, line width=0.5pt]
  \begin{tikzpicture}[auto]
      % Place nodes
      \node [block] (Z) {Treatment};
      \node [block, above right of = Z, xshift = 2cm, yshift = 1.5cm] (X1) {$X_1 \sim \text{Bernoulli}(0.6)$};
      \node [block, right of = X1, xshift = 2.5cm] (X2) {$X_2 \sim N(0,1)$};
      \node [block, right of = X2, xshift = 2.5cm] (X3) {$X_3 \sim \text{Gamma}(1,1)$};
      \node [block, below of = X2, yshift = -1.25cm] (T) {Survival Time};
      \node [block, right of = T, xshift = 5cm] (C) {Censoring Time};
      \node [block, below of = T, yshift = -1.25cm] (OT) {Observed Time};
      % Draw edges
      \path [line] (X1) -- (Z);
      \path [line] (X2) -- (Z);
      \path [line] (X2) -- (T);
      \path [line] (X3) -- (T);
      \path [line] (X1) -- (C);
      \path [line] (X3) -- (C);
      \path [line, color=red] (Z) -- (T);
      \path [line] (T) -- (OT);
      \path [line] (C) -- (OT);
  \end{tikzpicture}
```

Each component will be described in further detail in the following sections. 

## 3.1 Covariates 

As described in the previous diagram, we will be utilizing three covariates in this simulation study: $X_1$, $X_2$, $X_3$. These covariates have the following distributions: $X_1 \sim  \text{Bernoulli}(0.6)$, $X_2 \sim  N(0, 1)$, $X_3 \sim \text{Gamma}(1, 1)$ and will explicitly be defined to not be time dependent. These distributions were selected to represent different kinds of covariates that may appear in a real-world study and also to a variety of distributional behaviors.

In addition, these three covariates will all affect the critical measures of interest in this study (i.e., treatment, survival time, and censoring time). However, any given measure will only be affected by two of the three covariates and no two measures will be affected by the same combination of covariates.

## 3.2 True Propensity Score Model

The true propensity score model will be defined using the following logistic regression, taking in covariates $X_1$ and $X_2$: 
$$
\begin{aligned}
g^{-1}(E[Z = 1 | \mathbf{X}_i]) = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i},
\\
\text{where } g(\mathbf{\mathbf{X}_i}) = \frac{\exp(\boldsymbol{\beta}\mathbf{X}_i^T)}{1 + \exp(\boldsymbol{\beta}\mathbf{X}_i^T)} = E[Z = 1 | \mathbf{X}_i] = e(\mathbf{X}_i)
\end{aligned}
$$

In our simulation, $\beta_1, \beta_2$ will be held fixed at $\beta_1 = \beta_2 = 0.1$ for the setting of high overlap of overlap, $\beta_1 = \beta_2 = 1$ for the setting of medium overlap, and $\beta_1 = \beta_2  = 3$ for the setting of low overlap. For each of these settings, the $\beta_0$ that would produce an expected treatment proportion of 55% will be obtained through a numerical search over an interval.

```{r}
source(file = "misc_functions.R")
source(file = "propensity_score_functions.R")
```

```{r}
# covariate setting
n <- 100000
covs_df <- gen_cur_covs_tib(n = n)
cov_prop_high_overlap <- 
  gen_prop_scores(b = list(-0.05, 0.1, 0.1, 0.1), covs = covs_df) 


cov_prop_moderate_overlap <- 
  gen_prop_scores(b = list(-0.475, 1, 1, 1), covs = covs_df) 


cov_prop_low_overlap <- 
  gen_prop_scores(b = list(-1.43, 3, 3, 3), covs = covs_df) 
```

The distribution of propensity scores according to each of these settings can be visualized with the following plots, demonstrating that the levels of overlap between treatment groups have indeed been achieved:

```{r, eval=TRUE, fig.width = 8, fig.height = 1}
high_overlap_gg <- cov_prop_high_overlap %>% 
  ggplot(
  aes(x = prop_score, fill = as.factor(trt))
) + 
  geom_density(alpha = 0.3) +
  labs(x = "Propensity Score",
       y = "Frequency",
       title = "High Overlap",
       fill = "Treatment")

med_overlap_gg <- 
  cov_prop_moderate_overlap %>% 
  ggplot(
  aes(x = prop_score, fill = as.factor(trt))
) + 
  geom_density(alpha = 0.3) +
  labs(x = "Propensity Score",
       y = "Frequency",
       title = "Moderate Overlap",
       fill = "Treatment")

low_overlap_gg <- cov_prop_low_overlap %>% 
  ggplot(
  aes(x = prop_score, fill = as.factor(trt))
) + 
  geom_density(alpha = 0.3) +
  labs(x = "Propensity Score",
       y = "Frequency",
       title = "Low Overlap",
       fill = "Treatment")

patched_ggs <- high_overlap_gg + med_overlap_gg + low_overlap_gg 
patched_ggs + plot_layout(guides = "collect")
```

## 3.3 True Survival Time Model

The true survival time model will be defined using the following Cox-Weibull mode, taking in covariates $X_1$ and $X_3$: 
$$
\begin{aligned}
h(t|\mathbf{X}_i) &= h_0(t) \exp(L_i),
\\
\text{where } h_0(t) &= \lambda \nu t^{\nu-1}, \text{ and } L_i = a_0 Z_i + a_1 X_{1i} + a_3 X_{3i}
\\
S(T_i^S = t) &= \exp(-\lambda t^v \exp (L_i))
\\
 &= 1 - F(t)
\end{aligned}
$$

The survival time for subject $i$ will then be drawn from:
$$
\begin{aligned}
T_i^S = \Bigg(\frac{-\log(u_i^S)}{\lambda \exp(L_i)}\Bigg)^{1/\nu}, 
\\
\text{where } u_i^S \sim \text{Unif}(0, 1)
\end{aligned}
$$

From this model, the treatment effect will be determined by $\alpha_0$. Two settings of $\alpha_0$ were selected to correspond with a low and high treatment effect respectively. 

To obtain the true treatment effects from each of these settings, we started with the conditional distributions of survival time that would be obtained from the defined Cox-Weibull model:
$$
\begin{aligned}
T(1)\Big|(X_1, X_3) &\sim h_{Z = 1}(t|X_1, X_3) = \lambda v t^{v-1}\exp(\alpha_0 + \alpha_1 X_1 + \alpha_3 X_3)
\\
&= \lambda^*\nu t^{\nu - 1}, \text{ where } \lambda^* = \lambda  \exp(\alpha_0 + \alpha_1 X_1 + \alpha_3 X_3)
\\
T(0)\Big|(X_1, X_3) &\sim h_{Z = 0}(t|X_1, X_3) = \lambda v t^{v-1}\exp(\alpha_1 X_1 + \alpha_3 X_3)
\\
&= \lambda^*\nu t^{\nu - 1}, \text{ where } \lambda^* = \lambda  \exp(\alpha_1 X_1 + \alpha_3 X_3)
\end{aligned}
$$

Our target estimand is the restricted average causal effect at time = 5, or $E[S_1(5) - S_0(5)]$, which is unconditional on $(X_1, X_3)$. By the law of total expectations and iterated expectations, we see the following:
$$
\begin{aligned}
E[S_{T_i}(5)] &= E\Bigg[E\Big[I(T_i \ge 5) | X_1, X_3\Big]\Bigg]
\\
&= \int _{X_1, X_3} P\Big[T_i \ge 5 | X_1= x_1, X_3 = x_3]P(X_1 = x_1, X_3 = x_3)d\mu (x_1, x_3)
\\
&= \int _{X_1, X_3} P\Big[T_i \ge 5 | X_1= x_1, X_3 = x_3]P(X_1 = x_1) P(X_3 = x_3)d\mu (x_1, x_3)
\end{aligned}
$$

Let $X_{11}, ..., X_{1m} \sim P(X_1)$ and $X_{31}, ..., X_{3m} \sim P(X_3)$. Then, for a sufficiently large $m$, we note: 
$$
\begin{aligned}
P[T_i \ge 5] &= \int _{X_1, X_3}P\Big[T_i \ge 5 | X_1= x_1, X_3 = x_3]P(X_1 = x_1) P(X_3 = x_3)d\mu (x_1, x_3) 
\\
&\approx \sum_{j=1}^m P[T_i \ge 5|X_1 = x_{1j}, X_3 = x_{3j}]
\end{aligned}
$$

The true value estimates of $S_1(5) = P[T(1) \ge 5]$ and $S_0(5) = P[T(0) \ge 5]$ will be calculated by taking two samples of size $m = 1000000$ from the distributions of $X_1$ and $X_3$ to simulate the distributional behavior of each of these random variables, and sum the computed conditional probabilities of $P[T(1) \ge 5|(X_{1j}, X_{3j})]$ and $P[T(0) \ge 5|(X_{1k}, X_{3k})]$ for each sample $j$ and $k$, where $j = k = m$. 

The true RACE values that we computed from this procedure were $0.12$ for the "high" treatment effect setting and $0.014$ for the "low" treatment effect setting.

The distribution of survival time according to each of these settings and by levels of covariate overlap between treatment groups can be visualized with the following plots:

```{r}
source(file = "outcome_functions.R")
```

```{r, fig.width = 8, fig.height = 2, fig.asp = 0.4}
a0 <- -1
a1 <- 2
a3 <- 3
nu <- 3
lambda <- 0.0001

### test outcomes ###
out_test_po_1 <- 
  gen_outcomes(cov_prop_high_overlap, lambda = lambda, nu = nu)

out_gg_1 <- out_test_po_1 %>% 
  ggplot(
  aes(x = surv_time, fill = as.factor(trt))
) +
  geom_density(alpha=0.3) +
  labs(
    x = "Time",
    y = "Density",
    title = "High Overlap",
    fill = "Treatment"
  )

out_test_po_2 <- 
  gen_outcomes(cov_prop_moderate_overlap, lambda = lambda, nu = nu)

out_gg_2 <- out_test_po_2 %>% 
  ggplot(
  aes(x = surv_time, fill = as.factor(trt))
) +
  geom_density(alpha=0.3) +
  labs(
    x = "Time",
    y = "Density",
    title = "Moderate Overlap",
    fill = "Treatment"
  )

out_test_po_3 <- 
  gen_outcomes(cov_prop_low_overlap, lambda = lambda, nu = nu)

out_gg_3 <- out_test_po_3 %>% 
  ggplot(
  aes(x = surv_time, fill = as.factor(trt))
) +
  geom_density(alpha=0.3) +
  labs(
    x = "Time",
    y = "Density",
    title = "Low Overlap",
    fill = "Treatment"
  )

a0 <- -0.01
a1 <- 2
a3 <- 3
nu <- 3
lambda <- 0.0001

### test outcomes ###
out_test_po_4 <- 
  gen_outcomes(cov_prop_high_overlap, lambda = lambda, nu = nu)

out_gg_4 <- out_test_po_4 %>% 
  ggplot(
  aes(x = surv_time, fill = as.factor(trt))
) +
  geom_density(alpha=0.3) +
  labs(
    x = "Time",
    y = "Density",
    title = "High Overlap",
    fill = "Treatment"
  )

out_test_po_5 <- 
  gen_outcomes(cov_prop_moderate_overlap, lambda = lambda, nu = nu)

out_gg_5 <- out_test_po_5 %>% 
  ggplot(
  aes(x = surv_time, fill = as.factor(trt))
) +
  geom_density(alpha=0.3) +
  labs(
    x = "Time",
    y = "Density",
    title = "Moderate Overlap",
    fill = "Treatment"
  )
out_test_po_6 <- 
  gen_outcomes(cov_prop_low_overlap, lambda = lambda, nu = nu)

out_gg_6 <- out_test_po_6 %>% 
  ggplot(
  aes(x = surv_time, fill = as.factor(trt))
) +
  geom_density(alpha=0.3) +
  labs(
    x = "Time",
    y = "Density",
    title = "Low Overlap",
    fill = "Treatment"
  )

top_out_ggs <- (out_gg_1 + out_gg_2 + out_gg_3) 
bottom_out_ggs <- (out_gg_4 + out_gg_5 + out_gg_6)

top_out_ggs + plot_layout(guides = "collect") + plot_annotation(title = "Density of Survival Time by Treatment\nHigh Treatment Effect")

bottom_out_ggs + plot_layout(guides = "collect") + plot_annotation(title = "Density of Survival Time by Treatment\nLow Treatment Effect")
```

## 3.4 True Censoring Model

The censoring model will be defined using the following exponential model, taking in covariates $X_2$ and $X_3$, and is generated independently of $T_i^C$, $T_i^S$, and $Z_i$:
$$
\begin{aligned}
T^{C}_i \sim \text{Exponential}(\lambda \exp(K_i)),
\\
\text{where } K_i = \gamma_0 + \gamma_2 X_{2i} + \gamma_3 X_{3i}
\end{aligned}
$$


Algorithmically, the censoring time for subject $i$ will be drawn from:
$$
\begin{aligned}
T^{C}_i=\frac{-\log(u^{C})}{\lambda \exp(K_i)},
\\
\text{where } u^{C} \sim \text{Unif}(0, 1)
\end{aligned}
$$

For subject $i$, the observed time, $T^{obs}$ is the minimum of $T_i^C$ and $T_i^S$, or $T^{obs} = \min (T_i^C, T_i^S)$. In addition, the censoring indicator for subject $i$, given the survival time $T_i$ and censoring time $T^C$, will be assigned as follows:
$$
\begin{aligned}
\delta_i(T_i^S, T_i^C) = \begin{cases}
1, \quad \text{ where }T_i^S > T_i^C
\\
0. \quad \text{ where }T_i^S \le T_i^C
\end{cases}
\end{aligned}
$$
It is critical to note that although $T_i^C$ is by definition independent of $T_i^S$ and $Z_i$, the censoring indication, $\delta_i$ is not. 

In a similar fashion to how the model intercept was selected in the true propensity score model in order to fix a treatment proportion in expectation, after fixing $\gamma_2 = 2$ and $\gamma_3 = 4$, a numerical search procedure was also run to select a $\gamma_0$ so that the proportion of censored subjects could be varied across two levels, corresponding to low (25%) and high (50%) levels of censoring. The selected intercepts varied according to the magnitude of treatment effect, as well as the defined censoring level, consequentially resulting in four defined $\gamma_0$ values corresponding to the four treatment effect and censoring level categories. These were as follows: $\gamma_0=1.04$ for the high treatment effect with low censoring setting, $\gamma_0=1.2$ for the low treatment effect with low censoring setting, $\gamma_0=3.35$ for the high treatment effect with high censoring setting, and $\gamma_0=3.71$ for the low treatment effect with high censoring setting.

# 4. Results

In the conduct of this simulation study, data were randomly generated from the distributions described in the previous section to simulate random samples of size 1000. These datasets were then weighted through one of the four described weighting procedures, covariate balance was assessed through absolute standard difference (ASD), and weighted Kaplan-Meier estimates were computed to estimate $\Delta_{RACE}(5)$. Bootstrap replicates were drawn to compute the relative bias and empirical 95% coverage rates, and their results were summarized in tabular as well as graphical formats.

## 4.1 Separate Weighting

The following table and plot describe the covariate balance results from the two separate weights weighting procedures: 

```{r}
load("table2.RData")
table2_to_kable <-
  table2 %>% 
  as_tibble() %>%
  janitor::clean_names()

table2_to_kable$scenario_asd[100:108] <- "Low/Low/High"
table2_to_kable$scenario_asd[91:99] <- "Med/Low/High"
table2_to_kable$scenario_asd[82:90] <- "High/Low/High"

table2_to_kable %>%
  mutate(
    asd_all = round(as.numeric(asd_all), 4)
  ) %>%
  group_by(scenario_asd) %>%
  pivot_wider(
    names_from = names,
    values_from = asd_all
  ) %>%
  janitor::clean_names() %>%
  unnest(x1_asd_raw:x3_asd_wt2) %>%
  knitr::kable(
    col.names = linebreak(c("Scenario$^\\text{a}$",
                  "Unweighted",
                  "Dual Logistic",
                  "Logistic/Cox",
                  "Unweighted",
                  "Dual Logistic",
                  "Logistic/Cox",
                  "Unweighted",
                  "Dual Logistic",
                  "Logistic/Cox"
                  ), align="r"),
      booktabs=TRUE,
    escape=FALSE,
     caption = "Separate Weighting Covariate Balance Results (ASD)"
  )%>%
  kable_styling(latex_options = c("HOLD_position", "scale_down")) %>% 
  add_header_above(c(" " = 1, "X1" = 3, "X2"=3, "X3" = 3), escape = FALSE) %>%
  add_footnote(c("Note: scenarios are described by (degree of overlap/treatment effect level/censoring level)"))
```

```{r, fig.height = 1, fig.width = 8}
to_asd_gg <- table2_to_kable %>%
  group_by(scenario_asd) %>%
  pivot_wider(
    names_from = names,
    values_from = asd_all
  ) %>%
  janitor::clean_names() %>%
  unnest(x1_asd_raw:x3_asd_wt2) %>%
  pivot_longer(
    cols = x1_asd_raw:x3_asd_wt2
  ) %>%
  separate(
    name, into=c("var", "weight"),
    sep = "_asd_",
    extra="merge"
  )  %>%
  mutate(
    weight = case_when(
      weight == "raw" ~ "Unweighted",
      weight == "wt1" ~ "Dual Logistic",
      weight == "wt2" ~ "Logistic/Cox"
    ),
    scenario = scenario_asd,
    value = as.numeric(value)
  ) 

asd_gg_1 <- 
  to_asd_gg %>%
  filter(var=="x1") %>%
  ggplot(
    aes(x = scenario, 
    y = value,
    col = weight,
    group = weight)
  ) +
  labs(x = "Scenario", y = "ASD", col = "Weighting Procedure",
       title = "X1") +
  geom_point() + 
  geom_line() + 
  coord_flip()

asd_gg_2 <- 
  to_asd_gg%>%
  filter(var=="x2") %>%
  ggplot(
    aes(x = scenario, 
    y = value,
    col = weight,
    group = weight)
  ) +
  labs(x = "Scenario", y = "ASD", col = "Weighting Procedure",
       title = "X2") +
  geom_point() + 
  geom_line() + 
  coord_flip()

asd_gg_3 <- 
  to_asd_gg %>%
  filter(var=="x3") %>%
  ggplot(
    aes(x = scenario, 
    y = value,
    col = weight,
    group = weight)
  ) +
  labs(x = "Scenario", y = "ASD", col = "Weighting Procedure",
       title = "X3") +
  geom_point() + 
  geom_line() + 
  coord_flip()
  
(asd_gg_1 + asd_gg_2 + asd_gg_3) + plot_layout(guides = "collect") + plot_annotation(title = "ASD by Scenario and Weighting Procedure\nSeparate Weighting Procedures")
```

We note that the weighting method using logistic regression for IPTW and Cox model for IPCW out-performed the dual-logistic regression weighting method in terms of balancing baseline covariates, as the logistic/Cox method was consistently associated with lower ASDs than the dual-logistic regression method. Both of these methods produced better covariate balance than what existed in the unweighted data on all covariates excluding $X_3$.

The summarized results for the estimation of the causal estimand are summarized in the following table and plot:

```{r}
load("table1.RData")
table1_to_kable <-
  table1 %>% 
  as_tibble() %>%
  janitor::clean_names() %>%
  mutate(
    all_v1 = round(as.numeric(all_v1), 4),
    scenario =
      case_when(
        true_ace == 0.014 & scenario == "High/High/High" ~ "High/Low/High",
        true_ace == 0.014 & scenario == "Med/High/High" ~ "Med/Low/High",
        true_ace == 0.014 & scenario == "Low/High/High" ~ "Low/Low/High",
        TRUE ~ scenario
      )
  ) 
table1_to_kable %>%
  group_by(scenario, true_ace) %>%
  pivot_wider(
    names_from = c(v1, measure),
    values_from = all_v1
  ) %>%
  janitor::clean_names() %>%
  knitr::kable(
    col.names = c("Scenario$^\\text{a}$",
                  "True ACE",
                  "Avg. Estimate",
                  "Relative Bias",
                  "95\\% Coverage Rate",
                  "Avg. Estimate",
                  "Relative Bias",
                  "95\\% Coverage Rate"
                  ),
      booktabs=TRUE,
    escape=FALSE,
    caption = "Separate Weighting Estimation Results"
  )%>%
  kable_styling(latex_options = c("HOLD_position", "scale_down")) %>% 
  add_header_above(c(" " = 2, "Dual Logistic" = 3, "Logistic/Cox"=3), escape = FALSE) %>%
  add_footnote(c("Note: scenarios are described by (degree of overlap/treatment effect level/censoring level)"))
```

```{r, fig.width = 8, fig.height = 1}
to_seq_bias_gg <- 
  table1_to_kable %>%
  select(scenario, measure, all_v1, v1) %>%
  mutate(
    name = case_when(
      v1 == "Duel Logistic" ~ "Dual Logistic",
      v1 == "Logistic Cox" ~ "Logistic/Cox"
    ),
    value = all_v1
  ) %>%
  filter(measure == "Relative Bias")
  

to_seq_coverage_gg <- 
  table1_to_kable %>%
  select(scenario, measure, all_v1, v1) %>%
  mutate(
    name = case_when(
      v1 == "Duel Logistic" ~ "Dual Logistic",
      v1 == "Logistic Cox" ~ "Logistic/Cox"
    ),
    value = all_v1
  ) %>%
  filter(measure == "95% Coverage")

bias_gg_1 <- 
  to_seq_bias_gg %>%
  ggplot(
    aes(x = scenario, 
    y = value,
    col = name,
    group = name)
  ) +
  labs(x = "Scenario", y = "", col = "Weighting Procedure", title = "Relative Bias") +
  geom_hline(yintercept = 0, col="red", linetype="dashed") +
  geom_point() + 
  geom_line() + 
  coord_flip()

coverage_gg <- 
  to_seq_coverage_gg %>%
  ggplot(
    aes(x = scenario, 
    y = value,
    col = name,
    group = name)
  ) +
  labs(x = "Scenario", y = "", col = "Weighting Procedure", title = "95% Coverage Rates") +
  geom_hline(yintercept = 0.95, col="red", linetype="dashed") +
  geom_point() + 
  geom_line() + 
  coord_flip()

(bias_gg_1 + coverage_gg) + plot_layout(guides = "collect") + plot_annotation(title = "Bias and 95% Coverage by Scenario and Weighting Procedure\nSeparate Weighting Procedures")
```

When using logistic regression to estimate both IPTW and IPCW, we observed smaller relative bias and higher 95% coverage rate among low censoring scenarios regardless of overlap and treatment effect settings. The high overlap scenarios were also observed associated with higher 95% coverage rate. However, such a pattern was not obvious when using Cox model to estimate IPCW.

## 4.2 Sequential Weighting

The following table provides results corresponding to how the sequential weighting procedures performed in balancing covariates between the two treatment groups:

```{r}
seq_weights_asd <- read_csv(file = "fin_asd_sum.csv") %>%
  select(-scenario) %>%
  bind_cols(table1_to_kable %>% select(scenario) %>% unique()) %>%
  relocate(
    scenario
  )

seq_weights_asd %>% 
  select(
    scenario, 
    x1_none,
    x1_iptw_ipcw,
    x1_ipcw_iptw,
    x2_none,
    x2_iptw_ipcw,
    x2_ipcw_iptw,
    x3_none,
    x3_iptw_ipcw,
    x3_ipcw_iptw
  ) %>%
  mutate(
    x1_none = round(x1_none, 4),
    x1_iptw_ipcw = round(x1_iptw_ipcw, 4),
    x1_ipcw_iptw = round(x1_ipcw_iptw, 4),
    x2_none = round(x2_none, 4),
    x2_iptw_ipcw = round(x2_iptw_ipcw, 4),
    x2_ipcw_iptw = round(x2_ipcw_iptw, 4),
    x3_none = round(x3_none, 4),
    x3_iptw_ipcw = round(x3_iptw_ipcw, 4),
    x3_ipcw_iptw = round(x3_ipcw_iptw, 4)
  ) %>%
  knitr::kable(
    col.names = linebreak(c(
      "Scenario$^\\text{b}$", 
      "Unweighted",
      "IPTW$\\circ$IPCW",
      "IPCW$\\circ$IPTW",
      "Unweighted",
      "IPTW$\\circ$IPCW",
      "IPCW$\\circ$IPTW",
      "Unweighted",
      "IPTW$\\circ$IPCW",
      "IPCW$\\circ$IPTW"),
    align="r"),
    booktabs=TRUE,
    escape=FALSE,
    caption = "Sequential Weighting Covariate Balance Results (ASD)$^\\text{a}$"
  )%>%
  kable_styling(latex_options = c("HOLD_position", "scale_down")) %>% 
  add_header_above(c(" " = 1, "X1" = 3, "X2"= 3, "X3" = 3), escape = FALSE) %>%
  add_footnote(c("Averaged over 1000 total bootstrapped replicates (100 bootstrap replicates over each of 100 samples)", "Scenarios are described by (degree of overlap/treatment effect level/censoring level)"))
```

```{r, fig.height = 1, fig.width = 8}
to_asd_gg <- seq_weights_asd %>%
  pivot_longer(
    cols = x1_none:x3_ipcw_iptw
  ) %>%
  separate(
    name, into=c("var", "weight"),
    sep = "_",
    extra="merge"
  )  %>%
  mutate(
    weight = case_when(
      weight == "iptw" ~ "IPTW",
      weight == "ipcw" ~ "IPCW",
      weight == "iptw_ipcw" ~ "IPTW*IPCW",
      weight == "ipcw_iptw" ~ "IPCW*IPTW",
      weight == "none" ~ "Unweighted"
    )
  ) %>%
  filter(weight %in% c("IPTW*IPCW", "IPCW*IPTW", "Unweighted"))

asd_gg_1 <- 
  to_asd_gg%>%
  filter(var=="x1") %>%
  ggplot(
    aes(x = scenario, 
    y = value,
    col = weight,
    group = weight)
  ) +
  labs(x = "Scenario", y = "ASD", col = "Weighting Procedure",
       title = "X1") +
  geom_point() + 
  geom_line() + 
  coord_flip()

asd_gg_2 <- 
  to_asd_gg%>%
  filter(var=="x2") %>%
  ggplot(
    aes(x = scenario, 
    y = value,
    col = weight,
    group = weight)
  ) +
  labs(x = "Scenario", y = "ASD", col = "Weighting Procedure",
       title = "X2") +
  geom_point() + 
  geom_line() + 
  coord_flip()

asd_gg_3 <- 
  to_asd_gg%>%
  filter(var=="x3") %>%
  ggplot(
    aes(x = scenario, 
    y = value,
    col = weight,
    group = weight)
  ) +
  labs(x = "Scenario", y = "ASD", col = "Weighting Procedure",
       title = "X3") +
  geom_point() + 
  geom_line() + 
  coord_flip()
  
(asd_gg_1 + asd_gg_2 + asd_gg_3) + plot_layout(guides = "collect") + plot_annotation(title = "ASD by Scenario and Weighting Procedure\nSequential Weighting Procedures")
```

It is apparent that there is improvement across all simulation settings in covariate balance in the IPTW$\circ$IPCW weighting method over the unweighted observed data in covariates $X_1$ and $X_2$, though this is not the case in $X_3$. We note that the covariate balance improved for most cases through the IPCW$\circ$IPTW weighting method over the unweighted observed data in $X_1$ and $X_2$, with the exclusion of the high-overlap settings, but the improvement was not as large as the IPTW$\circ$IPCW procedure. In addition, there was greater imbalance in $X_3$ in the IPCW$\circ$IPTW procedure than both the IPTW$\circ$IPCW and unweighted procedures. 

Now, we will assess the results under these two weighting procedures:

```{r}
seq_weights_res <- read_csv(file = "fin_res_sum.csv") %>%
  select(-scenario) %>%
  bind_cols(table1_to_kable %>% select(scenario, true_ace) %>% unique()) %>%
  relocate(
    scenario, true_ace
  )

seq_weights_res %>% 
  select(
    scenario, true_ace, 
    avg_te_trt_cens_adj_km,
    relative_bias_trt_cens_adj_km,
    coverage_prop_trt_cens_adj_km,
    avg_te_cens_trt_adj_km,
    relative_bias_cens_trt_adj_km,
    coverage_prop_cens_trt_adj_km
  ) %>%
  mutate(
    avg_te_trt_cens_adj_km = round(avg_te_trt_cens_adj_km, 4),
    relative_bias_trt_cens_adj_km = round(relative_bias_trt_cens_adj_km, 4),
    avg_te_cens_trt_adj_km = round(avg_te_cens_trt_adj_km, 4),
    relative_bias_cens_trt_adj_km = round(relative_bias_cens_trt_adj_km, 4)
  ) %>%
  knitr::kable(
    col.names = c(
      "Scenario$^\\text{b}$", "True ACE",
      "Avg. Estimate",
      "Relative Bias",
      "95\\% Coverage Rate",
      "Avg. Estimate",
      "Relative Bias",
      "95\\% Coverage Rate"),
    booktabs=TRUE,
    escape=FALSE,
    caption = "Sequential Weighting Estimation Results$^\\text{a}$"
  )%>%
  kable_styling(latex_options = c("HOLD_position", "scale_down")) %>% 
  add_header_above(c(" " = 2, "IPTW*IPCW" = 3, "IPCW*IPTW"= 3), escape = FALSE) %>%
  add_footnote(c("Obtained over 1000 total bootstrapped replicates (100 bootstrap replicates over each of 100 samples)", "Note: scenarios are described by (degree of overlap/treatment effect level/censoring level)"))
```

```{r, fig.height = 1, fig.width = 8}
to_seq_bias_gg <- 
  seq_weights_res %>%
  select(scenario, relative_bias_trt_cens_adj_km, relative_bias_cens_trt_adj_km) %>%
  pivot_longer(
    cols = c(relative_bias_trt_cens_adj_km, relative_bias_cens_trt_adj_km)
  ) %>%
  mutate(
    name = case_when(
      name == "relative_bias_trt_cens_adj_km" ~ "IPTW*IPCW",
      name == "relative_bias_cens_trt_adj_km" ~ "IPCW*IPTW"
    )
  ) 

to_seq_coverage_gg <- 
  seq_weights_res %>%
  select(scenario, coverage_prop_trt_cens_adj_km, coverage_prop_cens_trt_adj_km) %>%
  pivot_longer(
    cols = c(coverage_prop_trt_cens_adj_km, coverage_prop_cens_trt_adj_km)
  ) %>%
  mutate(
    name = case_when(
      name == "coverage_prop_trt_cens_adj_km" ~ "IPTW*IPCW",
      name == "coverage_prop_cens_trt_adj_km" ~ "IPCW*IPTW"
    )
  ) 

bias_gg_1 <- 
  to_seq_bias_gg %>%
  ggplot(
    aes(x = scenario, 
    y = value,
    col = name,
    group = name)
  ) +
  labs(x = "Scenario", y = "", col = "Weighting Procedure", title = "Relative Bias") +
  geom_hline(yintercept = 0, col="red", linetype="dashed") +
  geom_point() + 
  geom_line() + 
  coord_flip()

coverage_gg <- 
  to_seq_coverage_gg %>%
  ggplot(
    aes(x = scenario, 
    y = value,
    col = name,
    group = name)
  ) +
  labs(x = "Scenario", y = "", col = "Weighting Procedure", title = "95% Coverage Rates") +
  geom_hline(yintercept = 0.95, col="red", linetype="dashed") +
  geom_point() + 
  geom_line() + 
  coord_flip()

(bias_gg_1 + coverage_gg) + plot_layout(guides = "collect") + plot_annotation(title = "Bias and 95% Coverage by Scenario and Weighting Procedure\nSequential Weighting Procedures")
```

Here, we see that the IPCW$\circ$IPTW seems to produce less relative bias than the IPTW$\circ$IPCW method overall. However, the IPTW$\circ$IPCW method indicates better empirical 95% confidence interval coverage rate than the IPCW$\circ$IPTW method, in which many scenarios had coverage rates that were very close to the aspiration of 95%. 

The IPTW$\circ$IPCW method seemed to indicate best performances under settings of high censoring and high treatment effect, irrespective of the degree of treatment overlap. There was less of an observable trend with the IPCW$\circ$IPTW method, but both methods had the best performance in the "all-high" setting, where the relative bias was indicated to be very small and the 95% confidence interval coverage rate was high (95% for the IPCW$\circ$IPTW method and approximately 100% for the IPTW$\circ$IPCW) method. 

In these two procedures, high levels of relative bias did not necessarily correspond with lower 95% coverage, and vice versa -- in fact, some of the scenarios in which bias for both sequential weighting methods was the lowest also corresponded with lower 95% coverage (specifically the High/High/Low setting), while the scenario that was associated with the highest relative bias for both methods still produced a reasonable 95% coverage in both methods (Low/Low/Low setting). 

## 4.3 Comparison of Procedures

When considering covariate balance, the two separate weighting procedures as well as the IPTW$\circ$IPCW sequential weighting procedure performed similarly. The IPCW$\circ$IPTW procedure produced the most sporadic balancing results out of the four weighting methods. 

These weighting procedures produced interesting results with respect to relative bias and 95% confidence interval coverage rates. Both of the sequential weighting procedures seemed to out-perform the separate weighting procedures in the 95% empirical coverage rates, with the lowest coverage in the sequential procedures being around 84% and the majority of which achieving around 92% coverage. 

In addition, it was interesting to see that the separate weighting procedures seemed to negatively bias the causal estimates while the sequential procedures seemed to positively bias estimates. The bias in the separate weighting procedures seemed to be more predictable in behavior than the sequential weighting procedures, as low levels of censoring in the separate weighting procedures always corresponded with lower levels of relative bias, holding all other settings constant. 

# 5. Discussion

In this project, we conducted a simulation studies to evaluate the performances of two weighting schemes under causal survival analysis framework. We specified 12 simulation scenarios which took different treatment effects, overlaps of covariates distribution, and censoring rates into consideration. Trends in the separate weighting procedure with logistic regressions for both IPTW and IPCW match what we expected, where we observed lower overlap and higher censoring rates scenarios corresponded with lower coverage rates. In addition, this procedure performed better in lower censoring settings than higher censoring counterparts. However, when using `ipcw` to calculate IPCW with Cox model, we found increases in relative bias and decreases in the coverage rates. For sequential weighting procedure, there were no clear trends in the sequential weighting procedure, where most of the relative bias ranged around 10% to 20% and most of the coverage rates were above 90%.  

However, the relative bias is, in general, lower than what we expected, which may be attributed to the following reasons. First of all, all the weighting models in our analysis were misspecified, because we tried to mimic the real world situation in which no one knew the true models. Secondly, due to the time constraint we didn't validate the `ipcw` function when using it, which may cause a mistaken usage of it.    

\newpage
# 6. References

Mao, H., Li, L., Yang, W., & Shen, Y. (2018). On the propensity score weighting analysis with survival outcome: Estimands, estimation, and inference. Statistics in medicine, 37(26), 3745-3763.

Cheng, C., Li, F., Thomas, L. E., & Li, F. (2022). Addressing extreme propensity scores in estimating counterfactual survival functions via the overlap weights. American journal of epidemiology, 191(6), 1140-1151.

Robins, J. M., Hernán, M. A., & Brumback, B. (2000). Marginal structural models and causal inference in epidemiology. Epidemiology (Cambridge, Mass.), 11(5), 550–560. https://doi.org/10.1097/00001648-200009000-00011

Andersen, P. K., & Perme, M. P. (2010). Pseudo-observations in survival analysis. Statistical Methods in Medical Research, 19(1), 71-99. 10.1177/0962280209105020

Zeng, S., Li, F., & Hu, L. (2021). Propensity score weighting analysis of survival outcomes using pseudo-observations. arXiv preprint arXiv:2103.00605.

Wang, C., Wei, K., Huang, C., Yu, Y., & Qin, G. (2023). Multiply robust estimator for the difference in survival functions using pseudo-observations. BMC medical research methodology, 23(1), 247. https://doi.org/10.1186/s12874-023-02065-6

Bender R, Augustin T and Blettner M. (2005). Generating survival times to simulate cox proportional hazards models. Statistics in Medicine; 24: 1713–1723. https://doi.org/10.1002/sim.2059.

