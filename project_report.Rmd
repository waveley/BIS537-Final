---
title: "BIS 537 Final Project Report"
author: "Can Meng and Waveley Qiu"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
geometry: margin=2cm
output: 
  bookdown::pdf_document2:
    toc: false
    number_sections: false
header-includes:
  - \usepackage{mathrsfs}
  - \usepackage{amsfonts}  
  - \usepackage{amsmath}
  - \usepackage{amsthm}
---

```{r setup, include=FALSE}
set.seed(20220417)
library(tidyverse)
library(bookdown)
library(latex2exp)

# set knitr defaults
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.align = "center",
  fig.width = 6,
  fig.asp   = .6,
  out.width = "90%",
  cache = FALSE
)

# set theme defaults
theme_set(
  theme_bw() +
    theme(
      legend.position = "bottom",
      plot.title    = element_text(hjust = 0.5),
      plot.subtitle = element_text(hjust = 0.5),
      plot.caption  = element_text(hjust = 0.0)
    )
)

# set color scale defaults
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill   = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete   = scale_fill_viridis_d
```

# Background

## Survival Analysis and Causal Inference

While the ideal setting to conduct causal inference on survival outcomes are randomized experiments, as is true for most areas of research, it is often the case that such an experimental setting is difficult or impossible to achieve. As such, when treatment and control groups are not exchangeable, it may be necessary to implement causal inference methods to address the absence of direct counterfactual proxies.

Time-to-event outcomes are of great interest to investigators across a variety of clinical settings, however present analytical difficulties even within the constrains of a randomized controlled trial. In particular, the non-observation of an event yet loss of a subject's data, or censoring, presents another missing data issue that the investigator has need to consider. 

In this study, we will investigate two possible methods in which this dual-level missing data issue can be addressed. First, is an iterated weighting method, in which weights based on the probability of treatment and the probability of censoring are calculated and used together in estimating the causal estimand of interest. We then will investigate a matching and weighting combination method, in which the probability of censoring based on baseline covariates will be used to match censored records with complete records and the matched records will be weighted based on their propensity scores. 

## Causal Survival Estimands

The estimand of interest for this simulation study will be the restricted average survival causal effect, defined as follows (Mao 2018):

$$
\begin{aligned}
  \Delta_{RACE} &= \frac{E[\omega(e_i)\min(T_{1i}, t^*)]}{E[\omega(e_i)]} - \frac{E[\omega(e_i)\min(T_{0i}, t^*)]}{E[\omega(e_i)]}
  \\
  &= \int_0^{t^*} S_1(t)dt - \int_0^{t^*} S_0(t)dt
\end{aligned}
$$

This estimand is interpreted as the average difference in survival time between the treatment and control groups, if both potential outcomes are observed, under the upper bound time restriction of $t^*$.

## Data Generation Settings

We will be using three covariates in this simulation study: $X_1$, $X_2$, $X_3$. These covariates have the following distributions: $X_1 \sim  \text{Bernoulli}(0.6)$, $X_2 \sim  N(0, 1)$, $X_3 \sim \text{Gamma}(1, 1)$. Patterned after the simulation in Mao 2018, we will define our true models as follows:

### Propensity Score Model

The propensity score model will be defined as the following logistic regression: 

$$
\begin{aligned}
g^{-1}(E[Z = 1 | \mathbf{X}_i]) = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i},
\\
\text{where } g(\mathbf{\mathbf{X}_i}) = \frac{\exp(\boldsymbol{\beta}\mathbf{X}_i^T)}{1 + \exp(\boldsymbol{\beta}\mathbf{X}_i^T)} = E[Z = 1 | \mathbf{X}_i] = e(\mathbf{X}_i)
\end{aligned}
$$

In our simulation, $\beta_1$, $\beta_2$, and $\beta_3$ will be varied to create settings of varying overlap (i.e., weak and strong overlaps), and $\beta_0$ will be varied to specify treatment proportions (i.e., low and high treatment proportions).

### Survival Times Model

The outcomes model will be defined as the following Cox-Weibull model: 

$$
\begin{aligned}
h(t|\mathbf{X}_i) &= h_0(t) \exp(L_i),
\\
\text{where } h_0(t) &= \lambda \nu t^{\nu-1}, \text{ and } L_i = a_0 Z_i + a_1 X_{1i} + a_3 X_{3i}
\end{aligned}
$$

Then, the survival time for subject $i$ is drawn from:

$$
T_i^S = \Bigg(\frac{-\log(u_i^S)}{\lambda \exp(L_i)}\Bigg)^{1/\nu}, 
\\
\text{where } u_i^S \sim \text{Unif}(0, 1)
$$

### Censoring Model

The censoring model will be defined as the following exponential model:

$$
\begin{aligned}
T^{C}_i \sim \text{Exponential}(\lambda K_i),
\\
\text{where } K_i = \gamma_0 + \gamma_2 X_{2i} + \gamma_3 X_{3i}
\end{aligned}
$$

For simplicity, note that $T_i^C$ is independent of $T_i^S$ as well as treatment $Z_i$. This indicates that we are making the assuming that a subject's censoring time is dependent only on baseline covariates and is not influenced by their actual survival time or treatment assigned treatment.

Algorithmically, the censoring time for subject $i$ will be drawn from:

$$
T^{C}_i=\frac{-\log(u^{C})}{\lambda \exp(K_i)},
\\
\text{where } u^{C} \sim \text{Unif}(0, 1)
$$

For subject $i$, the observed time, $T^{obs}$ is the minimum of $T_i^C$ and $T_i^S$ ($T^{obs} = \min (T_i^C, T_i^S)$), the censoring indicator for subject $i$, given the survival time $T_i$ and censoring time $T^C$ will be assigned as follows:

$$
\begin{aligned}
C_i(T_i^S, T_i^C) = \begin{cases}
1, \quad \text{ where }T_i^S > T_i^C
\\
0. \quad \text{ where }T_i^S \le T_i^C
\end{cases}
\end{aligned}
$$

# Data Simulation

## DAG

## Treatment and Propensity Score

The propensity score is defined as follows:

$$
\begin{aligned}
e( \mathbf{X}_i) &= P(Z = 1 |  \mathbf{X}_i) = E[Z = 1 |  \mathbf{X}_i]
\\
\boldsymbol{\beta} \mathbf{X}_i^T &= \text{logit}(E[Z = 1 | \mathbf{X}_i])
\\
&\implies E[Z = 1 |\mathbf{X}_i] = \frac{\exp(\boldsymbol{\beta} \mathbf{X}_i^T)}{1 + \exp(\boldsymbol{\beta} \mathbf{X}_i^T)}
\\
E\Big[P(Z = 1)\Big] &= E_X\Big[E_Z\Big(Z = 1 | \mathbf{X}_i\Big)\Big]
\\
&= E_X\Bigg[\frac{\exp(\boldsymbol{\beta} \mathbf{X}_i^T)}{1 + \exp(\boldsymbol{\beta} \mathbf{X}_i^T)}\Bigg] = p
\end{aligned}
$$

As described previously, the covariates $X_1$ and $X_2$ in this model will be drawn from  $\text{Bernoulli}(0.6)$ and $N(0,1)$ distributions, respectively. Then, we see that we will need to select three $\beta$ coefficients to satisfy the form $g^{-1}(E[Z = 1 | X]) = \beta_0 + \beta_1 X_1 + \beta_2 X_2$, where $g(\mathbf{X}) = \frac{\exp(\boldsymbol{\beta X}^T)}{1 + \exp(\boldsymbol{\beta X}^T)} = E[Z = 1 | X]$. 

For weak overlap, let $\beta_1 = \beta_2 = \beta_3 = 3$. For moderate overlap,  $\beta_1 = \beta_2 = \beta_3 = 1$. For strong overlap,  $\beta_1 = \beta_2 = \beta_3 = 0.1$. 

```{r}
source(file = "propensity_score_functions.R")
```

```{r}
# covariate setting
n <- 100000

cov_prop_high_overlap <- 
  gen_prop_scores(b = list(high_overlap[[1]], 0.1, 0.1, 0.1)) 


cov_prop_moderate_overlap <- 
  gen_prop_scores(b = list(moderate_overlap[[1]], 1, 1, 1)) 


cov_prop_low_overlap <- 
  gen_prop_scores(b = list(low_overlap[[1]], 3, 3, 3)) 
```

The following plots show the overlap of these three simulations:

```{r}
cov_prop_high_overlap %>% 
  ggplot(
  aes(x = prop_score, fill = as.factor(trt))
) + 
  geom_density(alpha = 0.3) +
  labs(x = "Propensity Score",
       y = "Frequency",
       title = "High Overlap",
       fill = "Treatment")

cov_prop_moderate_overlap %>% 
  ggplot(
  aes(x = prop_score, fill = as.factor(trt))
) + 
  geom_density(alpha = 0.3) +
  labs(x = "Propensity Score",
       y = "Frequency",
       title = "Moderate Overlap",
       fill = "Treatment")


cov_prop_low_overlap %>% 
  ggplot(
  aes(x = prop_score, fill = as.factor(trt))
) + 
  geom_density(alpha = 0.3) +
  labs(x = "Propensity Score",
       y = "Frequency",
       title = "Low Overlap",
       fill = "Treatment")
```

## Outcomes

The following generates the outcomes, drawn from a Cox-Weibull model.

```{r}
source(file = "outcome_functions.R")
```

```{r}
a0 <- 1
a1 <- 2
a3 <- 3
nu <- 3
lambda <- 0.0001

### test outcomes ###
out_test_po_1 <- 
  gen_outcomes(cov_prop_high_overlap, lambda = lambda, nu = nu)

out_test_po_1 %>% 
  ggplot(
  aes(x = surv_time, fill = as.factor(trt))
) +
  geom_density(alpha=0.3) +
  labs(
    x = "Time",
    y = "Density",
    title = "Density of Time by Treatment\nHigh Overlap Set",
    fill = "Treatment"
  )
```

## Censoring



## True Estimand

As we have previously established, $T(0)\Big|(X_1, X_3)$ and $T(1)\Big|(X_1, X_3)$ are drawn from the following models:

$$
\begin{aligned}
T(1)\Big|(X_1, X_3) &\sim h_{Z = 1}(t|X_1, X_3) = \lambda v t^{v-1}\exp(\alpha_0 + \alpha_1 X_1 + \alpha_3 X_3)
\\
&= \lambda^*\nu t^{\nu - 1}, \text{ where } \lambda^* = \lambda  \exp(\alpha_0 + \alpha_1 X_1 + \alpha_3 X_3)
\\
\\
T(0)\Big|(X_1, X_3) &\sim h_{Z = 0}(t|X_1, X_3) = \lambda v t^{v-1}\exp(\alpha_1 X_1 + \alpha_3 X_3)
\\
&= \lambda^*\nu t^{\nu - 1}, \text{ where } \lambda^* = \lambda  \exp(\alpha_1 X_1 + \alpha_3 X_3)
\end{aligned}
$$

Our target estimand is the average causal effect, or $E[T(1) - T(0)]$, which is unconditional on $(X_1, X_3)$. By the law of total expectations and iterated expectations, we see the following:

$$
\begin{aligned}
E[T_i] &= E\Bigg[E\Big[T_i | X_1, X_3)\Big]\Bigg]
\\
&= \int _{X_1, X_3} E\Big[T_i | X_1= x_1, X_3 = x_3]P(X_1 = x_1, X_3 = x_3)d\mu (x_1, x_3)
\\
&= \int _{X_1, X_3} E\Big[T_i | X_1= x_1, X_3 = x_3]P(X_1 = x_1) P(X_3 = x_3)d\mu (x_1, x_3)
\end{aligned}
$$

Let $X_{11}, ..., X_{1m} \sim P(X_1)$ and $X_{31}, ..., X_{3m} \sim P(X_3)$. Then, for $m$ sufficiently large, we note: 

$$
\begin{aligned}
E[T_i] &= \int _{X_1, X_3} E\Big[T_i | X_1= x_1, X_3 = x_3]P(X_1 = x_1) P(X_3 = x_3)d\mu (x_1, x_3) 
\\
&\approx \sum_{j=1}^m E[T_i|X_1 = x_{1j}, X_3 = x_{3j}]
\end{aligned}
$$

To obtain the true value estimates of $E[T(1)]$ and $E[T(0)]$, we will draw 1000000 samples each from the distributions of $X_1$ and $X_3$ to simulate the distributional behavior of each of these random variables, and sum the computed conditional expectations of $T(1)|(X_{1j}, X_{3j})$ and $T(0)|(X_{1k}, X_{3k})$ for each sample $j$ and $k$, where $j = k = 1000000$.

```{r, message = TRUE}
library(progress)
library(beepr)
library(patchwork)

run_true_expected_time <- function(n, params){

  alpha_0 <- params[[1]]
  alpha_1 <- params[[2]]
  alpha_2 <- params[[3]]
  alpha_3 <- params[[4]]
  
  lambda <- params[[5]]
  v <- params[[6]]
  
  ###### define confounders here #####
  x_1 <- rbernoulli(n, 0.6)
  x_2 <- rnorm(n)
  x_3 <- rnorm(n, 0, 0.5)

  ###### define l here #####
  l <- alpha_0 + alpha_1*x_1 + alpha_3*x_3 
  
  t <- c()
  
  pb <- progress_bar$new(total = n, format = "running sim of size :total... [:bar]   :percent completed; eta: :eta")
  
#  for(i in 1:n){
#    pb$tick()
    cur_samp <- runif(n)
    calculated_samp <- (-log(cur_samp)/(lambda*exp(l)))^(1/v)
    t <- mean(calculated_samp)
#  }
  
#  t_hat <- mean(t)
#  beep()
  return(t)
}

m <- 1000000
t_1_true_val <- run_true_expected_time(n = m, params = list(-1, 2, 1.5, 3, 0.0001, 3)) %>% round(digits = 3)
t_0_true_val <- run_true_expected_time(n = m, params = list(0, 2, 1.5, 3, 0.0001, 3)) %>% round(digits = 3)
t_1_true_val - t_0_true_val
```

From conducting this procedure, and under the current model parameters, we find that the estimated true value of $\tau_1 = E[T(1)]$ is `r t_1_true_val` and the estimated true value of $\tau_0 = E[T(0)]$ is `r t_0_true_val`. Thus, we have an estimated true treatment effect of $\tau = E[T(1) - T(0)] =$ `r t_1_true_val - t_0_true_val` in this current setting. 





